# 📺 YouTube Videos

## **1. Transformadores**

<table>
<tr>
<td><a href="https://www.youtube.com/watch?v=BGH2H209rwY"><img width="140px" src="https://i.ytimg.com/vi/BGH2H209rwY/mqdefault.jpg"></a></td>
<td><a href="https://www.youtube.com/watch?v=BGH2H209rwY">1.1. Procesamiento de lenguaje natural</a><br/></td>
</tr>
</table>

<table>
<tr>
<td><a href="https://www.youtube.com/watch?v=_2BT6OeysqU"><img width="140px" src="https://i.ytimg.com/vi/_2BT6OeysqU/mqdefault.jpg"></a></td>
<td><a href="https://www.youtube.com/watch?v=_2BT6OeysqU">1.2. Definición de transformador</a><br/></td>
</tr>
</table>

<table>
<tr>
<td><a href="https://www.youtube.com/watch?v=eDceXluHjro"><img width="140px" src="https://i.ytimg.com/vi/eDceXluHjro/mqdefault.jpg"></a></td>
<td><a href="https://www.youtube.com/watch?v=eDceXluHjro">1.3. Arquitectura base</a><br/></td>
</tr>
</table>

<table>
<tr>
<td><a href="https://www.youtube.com/watch?v=uOIO1zLhUlw"><img width="140px" src="https://i.ytimg.com/vi/uOIO1zLhUlw/mqdefault.jpg"></a></td>
<td><a href="https://www.youtube.com/watch?v=uOIO1zLhUlw">1.4. Preparación de los datos</a><br/></td>
</tr>
</table>

<table>
<tr>
<td><a href="https://www.youtube.com/watch?v=kymVyHIeN4o"><img width="140px" src="https://i.ytimg.com/vi/kymVyHIeN4o/mqdefault.jpg"></a></td>
<td><a href="https://www.youtube.com/watch?v=kymVyHIeN4o">1.5. Una cabeza</a><br/></td>
</tr>
</table>

------------

## **2. Transformadores (Parte II)**

<table>
<tr>
<td><a href="https://www.youtube.com/watch?v=nw4ysA340v4"><img width="140px" src="https://i.ytimg.com/vi/nw4ysA340v4/mqdefault.jpg"></a></td>
<td><a href="https://www.youtube.com/watch?v=nw4ysA340v4">2.1. Combinación de las cabezas</a><br/></td>
</tr>
</table>

<table>
<tr>
<td><a href="https://www.youtube.com/watch?v=OkgykIX2DTM"><img width="140px" src="https://i.ytimg.com/vi/OkgykIX2DTM/mqdefault.jpg"></a></td>
<td><a href="https://www.youtube.com/watch?v=OkgykIX2DTM">2.2. Preceptrón multicapa en el bloque residual</a><br/></td>
</tr>
</table>

<table>
<tr>
<td><a href="https://www.youtube.com/watch?v=W7D6cxB75KA"><img width="140px" src="https://i.ytimg.com/vi/W7D6cxB75KA/mqdefault.jpg"></a></td>
<td><a href="https://www.youtube.com/watch?v=W7D6cxB75KA">2.3. Decodificador del transformador</a><br/></td>
</tr>
</table>

<table>
<tr>
<td><a href="https://www.youtube.com/watch?v=qzEXH9z3--0"><img width="140px" src="https://i.ytimg.com/vi/qzEXH9z3--0/mqdefault.jpg"></a></td>
<td><a href="https://www.youtube.com/watch?v=qzEXH9z3--0">2.4. Características del entrenamiento</a><br/></td>
</tr>
</table>

<table>
<tr>
<td><a href="https://www.youtube.com/watch?v=s6ERQ0QzIHg"><img width="140px" src="https://i.ytimg.com/vi/s6ERQ0QzIHg/mqdefault.jpg"></a></td>
<td><a href="https://www.youtube.com/watch?v=s6ERQ0QzIHg">2.5. Flujo residual</a><br/></td>
</tr>
</table>


------------

## **Extras**
[The Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/)

[A Mathematical Framework for Transformer Circuits](https://transformer-circuits.pub/2021/framework/index.html)

[Transformer Attention Layer gradient](https://say-hello2y.github.io/2022-09-07/attention-gradient)


