{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propagación hacia adelante con vectores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consideremos una red neuronal:\n",
    "\n",
    "<img src=\"./Ejemplo_red.jpeg\">\n",
    "\n",
    "Recordemos que para obtener la activación de cada uno de los perceptrones debemos de sumar los productos entre las respectivas entradas y los pesos respectivos:\n",
    "\n",
    "$$\n",
    "        u=\\sum_{i=1}^{m}w_ix_i+b\n",
    "$$\n",
    "\n",
    "donde $x_1 ... x_m$ son las entradas, $w_i ... w_m$ son los pesos asignados a cada una de las entradas y $b$ es el sesgo.\n",
    "\n",
    "Posteriormente es necesario pasar todo por la función de activación, y eso es necesario para cada una de las neuronas y por cada uno de los vectores a los que queramos evaluar.\n",
    "\n",
    "Sin embargo, podemos realizar este proceso de una manera un poco más sencilla. Para ello usaremos la formula anterior, que es equivalente al producto punto entre los vectores $X$ y $W$.\n",
    "\n",
    "$$ [x_1,x_2,x3] \\bullet [w_1,w_2,w_3] = (x_1 * w_1) + (x_2*w_2) + (x_3*w_3) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ello primero vamos a crear los vectores de pesos. Notemos que si deseamos utilizar un único perceptron al cual evaluaremos, con un vector unidimensional sería suficiente, en cambio podemos usar un vector bidimensional para representar a cada una de las capas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========Valores del vector W1=========\n",
      "[[5.69169738e-04 3.25076151e-01 7.23396239e-01 3.58622420e-01\n",
      "  6.77318369e-01 4.86570403e-01 3.71325451e-01 4.57402449e-01]\n",
      " [5.51716148e-01 5.60487596e-01 9.31597157e-01 2.12967169e-01\n",
      "  3.54215598e-01 7.85087420e-01 2.04655611e-01 9.61426714e-01]]\n",
      "========Valores del vector W2=========\n",
      "[[0.20376328 0.69222989 0.95866394]\n",
      " [0.5439493  0.11885755 0.23509243]\n",
      " [0.53664814 0.40489995 0.39641361]\n",
      " [0.21548707 0.1848591  0.4771438 ]]\n",
      "========Valores del vector W3=========\n",
      "[[0.59062035 0.82992988 0.38641893 0.30043624 0.93575443]\n",
      " [0.70338096 0.35825549 0.60102055 0.86400894 0.88301388]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Función de activación\n",
    "def sigma(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "#Pesos de las capas para fines ilustrativos serán aleatorios.\n",
    "\n",
    "W1 = np.random.random((2,8))\n",
    "W2 = np.random.random((4,3))\n",
    "W3 = np.random.random((2,5))\n",
    "\n",
    "print('========Valores del vector W1=========')\n",
    "print(W1)\n",
    "print('========Valores del vector W2=========')\n",
    "print(W2)\n",
    "print('========Valores del vector W3=========')\n",
    "print(W3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notemos que los vectores $w$ que representan a los pesos que van dirigidos únicamente a un perceptrón se encuentran de manera horizontal.\n",
    "\n",
    "Generamos también un vector de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.14619979 0.76058092 0.62960342 0.60124885]\n",
      " [0.47190697 0.06589045 0.35799615 0.56834613]\n",
      " [0.68466065 0.29619745 0.65231196 0.290043  ]\n",
      " [0.09207721 0.61198589 0.58311121 0.71555022]\n",
      " [0.09391256 0.70937474 0.32536306 0.0470296 ]\n",
      " [0.66892542 0.90142551 0.89306482 0.54213275]\n",
      " [0.6966939  0.7411648  0.55510719 0.01813096]]\n"
     ]
    }
   ],
   "source": [
    "X = np.random.random((7,4))\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y agregamos el sesgo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         1.         1.         1.        ]\n",
      " [0.14619979 0.76058092 0.62960342 0.60124885]\n",
      " [0.47190697 0.06589045 0.35799615 0.56834613]\n",
      " [0.68466065 0.29619745 0.65231196 0.290043  ]\n",
      " [0.09207721 0.61198589 0.58311121 0.71555022]\n",
      " [0.09391256 0.70937474 0.32536306 0.0470296 ]\n",
      " [0.66892542 0.90142551 0.89306482 0.54213275]\n",
      " [0.6966939  0.7411648  0.55510719 0.01813096]]\n"
     ]
    }
   ],
   "source": [
    "X0 = np.vstack((np.ones((1,X.shape[1])),X))\n",
    "print(X0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con lo cual ya podmeos realizar el producto punto entre estas dos matrices. Recordemos que esta se lleva acabo realizando productos punto entre las filas de la primera matriz y las columnas de la segunda, para dar como resultado una nueva matriz con las dimensiones dadas por la primer dimensión de la primera y la segunda dimensión de la segunda matriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de la primer matriz.\n",
      "(2, 8)\n",
      "Dimensiones de la segunda matriz.\n",
      "(8, 4)\n",
      "Dimensiones de la matriz resultado.\n",
      "(2, 4)\n",
      "Datos de la matriz resultado.\n",
      "[[1.31012481 1.8351067  1.83693404 1.42831556]\n",
      " [2.13216097 2.77322957 2.55548142 1.89871145]]\n"
     ]
    }
   ],
   "source": [
    "A1 = np.dot(W1,X0)\n",
    "print('Dimensiones de la primer matriz.')\n",
    "print(W1.shape)\n",
    "print('Dimensiones de la segunda matriz.')\n",
    "print(X0.shape)\n",
    "print('Dimensiones de la matriz resultado.')\n",
    "print(A1.shape)\n",
    "print('Datos de la matriz resultado.')\n",
    "print(A1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta manera también podemos aprovechar para aplicar la función de activación a todos los elementos resultantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.78753404 0.86236896 0.8625857  0.80663873]\n",
      " [0.89398998 0.94121194 0.9279409  0.86974562]]\n"
     ]
    }
   ],
   "source": [
    "A1 = sigma(A1)\n",
    "print(A1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y volviendo a realizar los mismos pasos podemos obtener los resultados de las siguientes capas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elementos de entrada para la siguiente capa.\n",
      "[[1.         1.         1.         1.        ]\n",
      " [0.78753404 0.86236896 0.8625857  0.80663873]\n",
      " [0.89398998 0.94121194 0.9279409  0.86974562]]\n",
      "Aplicación de los siguientes pesos\n",
      "[[1.60595384 1.70302679 1.69045436 1.59593647]\n",
      " [0.84772395 0.86772017 0.86462601 0.84429502]\n",
      " [1.20991042 1.2589305  1.25375744 1.20803511]\n",
      " [0.78763168 0.82399726 0.81770513 0.77959531]]\n",
      "Aplicación de la función de activación\n",
      "[[0.83284887 0.84592964 0.8442839  0.83144968]\n",
      " [0.70008947 0.70427109 0.70362625 0.69936902]\n",
      " [0.7702831  0.77884194 0.77794961 0.7699511 ]\n",
      " [0.68732258 0.69508419 0.69374899 0.68559289]]\n",
      "Elementos de entrada para la siguiente capa\n",
      "[[1.         1.         1.         1.        ]\n",
      " [0.83284887 0.84592964 0.8442839  0.83144968]\n",
      " [0.70008947 0.70427109 0.70362625 0.69936902]\n",
      " [0.7702831  0.77884194 0.77794961 0.7699511 ]\n",
      " [0.68732258 0.69508419 0.69374899 0.68559289]]\n",
      "Aplicación de los siguientes pesos\n",
      "[[2.42694044 2.44924676 2.44611423 2.42378251]\n",
      " [2.69496866 2.71641668 2.71348954 2.69222019]]\n",
      "Aplicación de la función de activación y resultado\n",
      "[[0.91885871 0.92050635 0.92027683 0.91862295]\n",
      " [0.9367291  0.93798843 0.93781795 0.93656601]]\n"
     ]
    }
   ],
   "source": [
    "print('Elementos de entrada para la siguiente capa.')\n",
    "X1 = np.vstack(( np.ones((1,A1.shape[1])) ,A1))\n",
    "print(X1)\n",
    "print('Aplicación de los siguientes pesos')\n",
    "A2 = np.dot(W2,X1)\n",
    "print(A2)\n",
    "print('Aplicación de la función de activación')\n",
    "A2 = sigma(A2)\n",
    "print(A2)\n",
    "print('Elementos de entrada para la siguiente capa')\n",
    "X2 = np.vstack((np.ones((1,A2.shape[1])),A2))\n",
    "print(X2)\n",
    "print('Aplicación de los siguientes pesos')\n",
    "A3 = np.dot(W3,X2)\n",
    "print(A3)\n",
    "print('Aplicación de la función de activación y resultado')\n",
    "A3 = sigma(A3)\n",
    "print(A3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las principales ventajas que podemos ver de utilizar vectores en lugar de realizar una a una las funciones, son que generan más claridad en cada uno de los pasos; que podemos aprovechar algoritmos e infraestructuras especializadas para realizar operaciones con matrices; que tenemos una estructura más concreta para la red neuronal"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
