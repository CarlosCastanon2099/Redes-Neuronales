<div align="center">

#  üìú Teor√≠a üóùÔ∏è

##   Curso de Redes Neuronales 2024-2

### <em>  Teor√≠a dada durante el curso: </em>

</div>

<div align="center">

[![](https://media3.giphy.com/media/v1.Y2lkPTc5MGI3NjExazBmeTNrMWZzODc3YXpueGJ3dGF2NTU1ZzR5ODY2OTdscmphOGE4MSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/1EuLa4HzCWffO/giphy.webp)](https://www.youtube.com/watch?v=zHNWEfES6XI)

</div>

```Haskell
\Redes Neuronales> Codigo-Teoria
```


```Haskell
\Redes Neuronales> Presentaciones-Material Extra
```

----------------

# **Extra**

# Sitios de inter√©s

Recuperados de: https://sites.google.com/ciencias.unam.mx/redesneuronales/p%C3%A1gina-principal

**Libro de texto escrito por los pioneros en aprendizaje profundo**

[**Deep learninig - Ian Goodfellow and Yoshua Bengio and Aaron Courville** ](https://www.deeplearningbook.org/)

Otro libro de texto sobre aprendizaje profundo en l√≠nea:

[Neural Networks and Deep Learning, Michael](http://neuralnetworksanddeeplearning.com/) Nielsen.

En particular poner atenci√≥n al cap√≠tulo:

[A visual proof that neural nets can compute any function](http://neuralnetworksanddeeplearning.com/chap4.html)

Diapositivas con material introductorio: <http://www.cacs.louisiana.edu/~maida/Classes/csce588/chapter6_supervisedLearning_weekOfMar30.pdf>

Libro en l√≠nea, del profesor Rojas. Es descargable. Faltan unas pocas im√°genes, pero al parecer no son esenciales: 

<https://page.mi.fu-berlin.de/rojas/neural/>

M√©todos de optimizaci√≥n num√©rica:

[Numerical Optimization, Nocedal](http://nasport.pmf.ni.ac.rs/materijali/2271/Numerical_Optimization%20Nocedal.pdf)

Libro en l√≠nea sobre redes neuronales y aprendizaje profundo de Michael Nielsen.

[Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/)

Sube tus datos y la computadora realiza el entrenamiento de un clasificador.

[Teachable machine](https://teachablemachine.withgoogle.com/)![](Aspose.Words.0d289ad9-1432-4407-b341-22667ec74e39.004.png)

Visualizaci√≥n din√°mica de redes neuronales sencillas

[Playground tensorflow](http://playground.tensorflow.org/)

Explicaci√≥n ilustrada de los fundamentos para un perceptr√≥n multicapa.

[One LEGO at a time: Explaining the Math of How Neural Networks Learn](https://omar-florez.github.io/scratch_mlp/)

## **Funciones de activaci√≥n**

An√°lisis de las opciones m√°s recientes: ventajas, desventajas y d√≥nde utilizar cada una. 

[Activation Functions in Neural Networks \[12 Types & Use Cases\]](https://www.v7labs.com/blog/neural-networks-activation-functions)

## **Cursos en l√≠nea**

Curso en l√≠nea del profesor Geoffrey Hinton: <https://class.coursera.org/neuralnets-2012-001>

Redes neuronales convolucionales para reconocimiento de im√°genes. Incluye redes neuronales recurrentes y aprendizaje por refuerzo. Colecci√≥n de videos de un curso de Stanford del 2017.

[Lecture Collection | Convolutional Neural Networks for Visual Recognition (Spring 2017)](https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv)

## **Redes neuronales como sistemas din√°micos**

Reporte sobre trabajos que estudian a las redes neuronales como sistemas din√°micos: <http://arxiv.org/pdf/0901.2203v2.pdf>

Explicaci√≥n en detalle del modelo de Hodkin-Huxley <http://nelson.beckman.illinois.edu/courses/physl317/part1/Lec3_HHsection.pdf>

"The Hodking-Husley Model" en Biological Signal Processing, Richard B. Wells <http://www.mrc.uidaho.edu/~rwells/techdocs/Biological%20Signal%20Processing/Chapter%2003%20The%20Hodgkin-Huxley%20Model.pdf>

Galer√≠a de un curso de Neurobiolog√≠a Computacional, con ejercicios. <http://nelson.beckman.illinois.edu/courses/physl317/image_gallery.html>

## **Entrenamiento con algoritmos gen√©ticos**

[Deep Neuroevolution: Genetic Algorithms are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning](https://arxiv.org/pdf/1712.06567.pdf)

## **Redes neuronales convolucionales**

P√°gina interactiva para ver el efecto de varios filltros al aplicar la convoluci√≥n. [Image Processing Convolutions.](http://beej.us/blog/data/convolution-image-processing/)

Presentaci√≥n te√≥rica sobre el uso de las RNC, de la Universidad de Stanford. [Convolutional Neural Networks for Visual Recognition](http://cs231n.github.io/convolutional-networks/)

T√©cnicas para visualizar qu√© aprendieron a detectar las redes

[Feature Visualization](https://distill.pub/2017/feature-visualization/)

## **Redes neuronales convolucionales sobre gr√°ficas**

Una propuesta para extraer informaci√≥n de gr√°ficas.

[How to do Deep Learning on Graphs with Graph Convolutional Networks ](https://towardsdatascience.com/how-to-do-deep-learning-on-graphs-with-graph-convolutional-networks-7d2250723780)

Aprendizaje semisupervisado

[Part 2: Semi-Supervised Learning with Spectral Graph Convolutions](https://towardsdatascience.com/how-to-do-deep-learning-on-graphs-with-graph-convolutional-networks-62acf5b143d0)

## **Redes neuronales recurrentes**

[Attention and Augmented Recurrent Neural Networks](https://distill.pub/2016/augmented-rnns/)

C√≥mo implementar atenci√≥n con Keras

[Adding A Custom Attention Layer To Recurrent Neural Network In Keras](https://machinelearningmastery.com/adding-a-custom-attention-layer-to-recurrent-neural-network-in-keras/)

Aplicaci√≥n de varias redes recurrentes al problema de buscar en una base de datos. 

[Neural Programmer: INDUCING LATENT PROGRAMS WITH GRADIENT DESCENT](https://arxiv.org/pdf/1511.04834)

## **Memoria a largo y corto plazo (LSTM)**

[Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)

## **Transformers** <img src="https://i.redd.it/7aq5n684wvq81.gif" width="45">

[Attention is all you need](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)

## **Redes neuronales para problemas de regresi√≥n**

Art√≠culo resumen sobre el planteamiento de problemas de regresi√≥n utilizando redes neuronales:

[Many regression algorithms, one unified model: A review, Freek Stulp and Olivier Sigaud](https://www.sciencedirect.com/science/article/abs/pii/S0893608015001185)

Repaso sobre transformadas de Fourier

http://www.thefouriertransform.com/


## **Aprendizaje profundo**

Diapositivas: Lo que el aprendizaje profundo significa para la inteligencia artificial 

[https://es.slideshare.net/jmugan/deep-learningforartificialintelligence](https://es.slideshare.net/jmugan/deep-learningforartificialintelligence)

Tutorial en l√≠nea de Juan C. Cuevas-Tello, Manuel Valenzuela-Rendon y Juan A. Nolazco-Flores <http://arxiv.org/abs/1603.07249>

Sesiones de la Escuela de Verano en Aprendizaje Profundo 2015 en Montreal, Canad√°. <http://videolectures.net/deeplearning2015_montreal/>

## **Redes de Hopfield**

[Hopfield network](http://www.scholarpedia.org/article/Hopfield_network)

## **M√°quinas de Boltzmann**

[A Beginner's Guide to Restricted Boltzmann Machines (RBMs)](https://skymind.ai/wiki/restricted-boltzmann-machine)

## **Herramientas**

Redes para reconocimiento visual

[YOLO: Real-Time Object Detection](https://pjreddie.com/darknet/yolo/)

## **PyTorch**

Tutorial introductorio a PyTorch <https://pytorch.org/tutorials/beginner/pytorch_with_examples.html#nn-module>

Un poco m√°s de detalles sobre c√≥mo PyTorch calcula los gradientes autom√°ticamente <https://pytorch.org/docs/stable/notes/autograd.html>

Resaltando:

*"Autograd is reverse automatic differentiation system. Conceptually, autograd records a graph recording all of the operations that created the data as you execute operations, giving you a directed acyclic graph whose leaves are the input tensors and roots are the output tensors. By tracing this graph from roots to leaves, you can automatically compute the gradients using the chain rule."*

## **Tensor Board**

[jhui.github.io/2017/03/12/TensorBoard-visualize-your-learning/ ](https://jhui.github.io/2017/03/12/TensorBoard-visualize-your-learning/)

[Tensorboard tutorial](https://neptune.ai/blog/tensorboard-tutorial)

## **Mapas de Kohonen**

Los mapas auto-organizados de Kohonen (SOM) 

<http://halweb.uc3m.es/esp/Personal/personas/jmmarin/esp/DM/tema5dm.pdf>

## **Redes adversarias generativas (GAN)**

Video que explica y muestra StyleGAN v2, desarrollada por NVidia. 

[YouTube StyleGANv2 Explained!](https://www.youtube.com/watch?v=u8qPvzk0AfY)

<table>
<tr>
<td><a href="https://www.youtube.com/watch?v=u8qPvzk0AfY"><img width="140px" src="https://i.ytimg.com/vi/u8qPvzk0AfY/mqdefault.jpg"></a></td>
<td><a href="https://www.youtube.com/watch?v=u8qPvzk0AfY"> StyleGANv2 Explained!</a><br/></td>
</tr>
</table>

Espacio latente

[El espacio latente en la IA](https://www.europeanvalley.es/noticias/el-espacio-latente-en-la-ia/)

## **Redes sobre grafos** 

<https://distill.pub/2021/gnn-intro/>

## **Campo de resplandor neuronal (*Neural radiance neld NeRF*)**

Representan una escena est√°tica como una funci√≥n continua 5D que devuelve la radianza emitida en cada direcci√≥n (Œ∏, œÜ) desde cada punto (x, y, z) en el espacio seg√∫n el m√©todo de trazado de rayos. El m√©todo optimiza un perceptr√≥n multicapa.

- [Neural Radiance Field (NeRF): A Gentle Introduction](https://datagen.tech/guides/synthetic-data/neural-radiance-field-nerf/)
- [Representing Scenes as Neural Radiance Fields for View Synthesis](https://www.matthewtancik.com/nerf)
- [NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis](https://arxiv.org/pdf/2003.08934.pdf)

## **Redes para procesamiento de audio**

[A Guide to DeepSpeech Speech to Text](https://medium.com/plain-simple-software/a-guide-to-deepspeech-speech-to-text-b4b051477cfa)

Fuente: https://deepspeech.readthedocs.io/en/r0.9/?badge=latest 


# **Videos de inter√©s**

## **Curso Redes Neuronales**

<table>
<tr>
<td><a href="https://www.youtube.com/watch?v=-KJU_o8MEck&list=PLdFhsX0Ypsns_d5I5nTiQ329gO4v0nyeH"><img width="140px" src="https://i.ytimg.com/vi/EfzMt6YurOA/mqdefault.jpg"></a></td>
<td><a href="https://www.youtube.com/watch?v=-KJU_o8MEck&list=PLdFhsX0Ypsns_d5I5nTiQ329gO4v0nyeH"> Curso Redes Neuronales</a><br/></td>
</tr>
</table>

## **Curso sobre redes neuronales convolucionales en Stanford**

<table>
<tr>
<td><a href="https://www.youtube.com/watch?v=vT1JzLTH4G4&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=1"><img width="140px" src="https://i.ytimg.com/vi/vT1JzLTH4G4/mqdefault.jpg"></a></td>
<td><a href="https://www.youtube.com/watch?v=vT1JzLTH4G4&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=1"> Neural Networks</a><br/></td>
</tr>
</table>


## **Redes adversarias generativas (GAN)**

<table>
<tr>
<td><a href="https://www.youtube.com/watch?v=u8qPvzk0AfY"><img width="140px" src="https://i.ytimg.com/vi/u8qPvzk0AfY/mqdefault.jpg"></a></td>
<td><a href="https://www.youtube.com/watch?v=u8qPvzk0AfY"> StyleGANv2 Explained!</a><br/></td>
</tr>
</table>

## **Deep Neural Networks are Easily Fooled**

<table>
<tr>
<td><a href="https://www.youtube.com/watch?v=M2IebCN9Ht4"><img width="140px" src="https://i.ytimg.com/vi/M2IebCN9Ht4/mqdefault.jpg"></a></td>
<td><a href="https://www.youtube.com/watch?v=M2IebCN9Ht4"> Deep Neural Networks are Easily Fooled</a><br/></td>
</tr>
</table>












