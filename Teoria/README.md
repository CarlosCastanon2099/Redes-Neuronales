<div align="center">

#  üìú Teor√≠a üóùÔ∏è

##   Curso de Redes Neuronales 2024-2

### <em>  Teor√≠a dada durante el curso: </em>

</div>

<div align="center">

[![](https://media3.giphy.com/media/v1.Y2lkPTc5MGI3NjExazBmeTNrMWZzODc3YXpueGJ3dGF2NTU1ZzR5ODY2OTdscmphOGE4MSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/1EuLa4HzCWffO/giphy.webp)](https://www.youtube.com/watch?v=zHNWEfES6XI)

</div>

```Haskell
\Redes Neuronales> Codigo-Teoria
```


```Haskell
\Redes Neuronales> Presentaciones-Material Extra
```

----------------

# **Extra**

# Sitios de inter√©s

Recuperados de: https://sites.google.com/ciencias.unam.mx/redesneuronales/p%C3%A1gina-principal

**Libro de texto escrito por los pioneros en aprendizaje profundo**

[**Deep learninig - Ian Goodfellow and Yoshua Bengio and Aaron Courville** ](https://www.deeplearningbook.org/)

Otro libro de texto sobre aprendizaje profundo en l√≠nea:

[Neural Networks and Deep Learning, Michael](http://neuralnetworksanddeeplearning.com/) Nielsen.

En particular poner atenci√≥n al cap√≠tulo:

[A visual proof that neural nets can compute any function](http://neuralnetworksanddeeplearning.com/chap4.html)

Diapositivas con material introductorio: <http://www.cacs.louisiana.edu/~maida/Classes/csce588/chapter6_supervisedLearning_weekOfMar30.pdf>

Libro en l√≠nea, del profesor Rojas. Es descargable. Faltan unas pocas im√°genes, pero al parecer no son esenciales: 

<https://page.mi.fu-berlin.de/rojas/neural/>

M√©todos de optimizaci√≥n num√©rica:

[Numerical Optimization, Nocedal](http://nasport.pmf.ni.ac.rs/materijali/2271/Numerical_Optimization%20Nocedal.pdf)

Libro en l√≠nea sobre redes neuronales y aprendizaje profundo de Michael Nielsen.

[Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/)

Sube tus datos y la computadora realiza el entrenamiento de un clasificador.

[Teachable machine](https://teachablemachine.withgoogle.com/)![](Aspose.Words.0d289ad9-1432-4407-b341-22667ec74e39.004.png)

Visualizaci√≥n din√°mica de redes neuronales sencillas

[Playground tensorflow](http://playground.tensorflow.org/)

Explicaci√≥n ilustrada de los fundamentos para un perceptr√≥n multicapa.

[One LEGO at a time: Explaining the Math of How Neural Networks Learn](https://omar-florez.github.io/scratch_mlp/)

## **Funciones de activaci√≥n**

An√°lisis de las opciones m√°s recientes: ventajas, desventajas y d√≥nde utilizar cada una. 

[Activation Functions in Neural Networks \[12 Types & Use Cases\]](https://www.v7labs.com/blog/neural-networks-activation-functions)

## **Cursos en l√≠nea**

Curso en l√≠nea del profesor Geoffrey Hinton: <https://class.coursera.org/neuralnets-2012-001>

Redes neuronales convolucionales para reconocimiento de im√°genes. Incluye redes neuronales recurrentes y aprendizaje por refuerzo. Colecci√≥n de videos de un curso de Stanford del 2017.

[Lecture Collection | Convolutional Neural Networks for Visual Recognition (Spring 2017)](https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv)

## **Redes neuronales como sistemas din√°micos**

Reporte sobre trabajos que estudian a las redes neuronales como sistemas din√°micos: <http://arxiv.org/pdf/0901.2203v2.pdf>

Explicaci√≥n en detalle del modelo de Hodkin-Huxley <http://nelson.beckman.illinois.edu/courses/physl317/part1/Lec3_HHsection.pdf>

"The Hodking-Husley Model" en Biological Signal Processing, Richard B. Wells <http://www.mrc.uidaho.edu/~rwells/techdocs/Biological%20Signal%20Processing/Chapter%2003%20The%20Hodgkin-Huxley%20Model.pdf>

Galer√≠a de un curso de Neurobiolog√≠a Computacional, con ejercicios. <http://nelson.beckman.illinois.edu/courses/physl317/image_gallery.html>

## **Entrenamiento con algoritmos gen√©ticos**

[Deep Neuroevolution: Genetic Algorithms are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning](https://arxiv.org/pdf/1712.06567.pdf)

## **Redes neuronales convolucionales**

P√°gina interactiva para ver el efecto de varios filltros al aplicar la convoluci√≥n. [Image Processing Convolutions.](http://beej.us/blog/data/convolution-image-processing/)

Presentaci√≥n te√≥rica sobre el uso de las RNC, de la Universidad de Stanford. [Convolutional Neural Networks for Visual Recognition](http://cs231n.github.io/convolutional-networks/)

T√©cnicas para visualizar qu√© aprendieron a detectar las redes

[Feature Visualization](https://distill.pub/2017/feature-visualization/)

## **Redes neuronales convolucionales sobre gr√°ficas**

Una propuesta para extraer informaci√≥n de gr√°ficas.

[How to do Deep Learning on Graphs with Graph Convolutional Networks ](https://towardsdatascience.com/how-to-do-deep-learning-on-graphs-with-graph-convolutional-networks-7d2250723780)

Aprendizaje semisupervisado

[Part 2: Semi-Supervised Learning with Spectral Graph Convolutions](https://towardsdatascience.com/how-to-do-deep-learning-on-graphs-with-graph-convolutional-networks-62acf5b143d0)

## **Redes neuronales recurrentes**

[Attention and Augmented Recurrent Neural Networks](https://distill.pub/2016/augmented-rnns/)

C√≥mo implementar atenci√≥n con Keras

[Adding A Custom Attention Layer To Recurrent Neural Network In Keras](https://machinelearningmastery.com/adding-a-custom-attention-layer-to-recurrent-neural-network-in-keras/)

Aplicaci√≥n de varias redes recurrentes al problema de buscar en una base de datos. 

[Neural Programmer: INDUCING LATENT PROGRAMS WITH GRADIENT DESCENT](https://arxiv.org/pdf/1511.04834)

## **Memoria a largo y corto plazo (LSTM)**

[Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)

## **Transformers** <img src="https://i.redd.it/7aq5n684wvq81.gif" width="45">

[Attention is all you need](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)

## **Redes neuronales para problemas de regresi√≥n**


